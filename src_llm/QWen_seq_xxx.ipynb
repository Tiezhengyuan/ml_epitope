{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9874d7de-dd10-4a0f-827a-13d76fac902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-7B-Instruct\n",
    "# unsloth/Qwen2.5-VL-72B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-72B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-72B-Instruct\n",
    "# unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-VL-3B-Instruct\n",
    "# unsloth/Qwen2.5-Math-7B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-7B-Instruct\n",
    "# unsloth/Qwen2.5-Math-7B\n",
    "# unsloth/Qwen2.5-Math-72B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-72B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-72B-Instruct\n",
    "# unsloth/Qwen2.5-Math-72B\n",
    "# unsloth/Qwen2.5-Math-1.5B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-1.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Math-1.5B-Instruct\n",
    "# unsloth/Qwen2.5-Math-1.5B\n",
    "# unsloth/Qwen2.5-Coder-7B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-7B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-7B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-7B\n",
    "# unsloth/Qwen2.5-Coder-3B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-3B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-3B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-3B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-3B\n",
    "# unsloth/Qwen2.5-Coder-32B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-32B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-32B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-32B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-32B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-32B\n",
    "# unsloth/Qwen2.5-Coder-14B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-14B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-14B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-14B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-14B\n",
    "# unsloth/Qwen2.5-Coder-1.5B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-1.5B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-1.5B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-1.5B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-1.5B\n",
    "# unsloth/Qwen2.5-Coder-0.5B-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-Coder-0.5B-Instruct-GGUF\n",
    "# unsloth/Qwen2.5-Coder-0.5B-Instruct-128K-GGUF\n",
    "# unsloth/Qwen2.5-Coder-0.5B-Instruct\n",
    "# unsloth/Qwen2.5-Coder-0.5B\n",
    "# unsloth/Qwen2.5-7B-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-Instruct-1M-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-Instruct-1M-bnb-4bit\n",
    "# unsloth/Qwen2.5-7B-Instruct-1M\n",
    "# unsloth/Qwen2.5-7B-Instruct\n",
    "# unsloth/Qwen2.5-7B\n",
    "# unsloth/Qwen2.5-72B-bnb-4bit\n",
    "# unsloth/Qwen2.5-72B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-72B-Instruct\n",
    "# unsloth/Qwen2.5-72B\n",
    "# unsloth/Qwen2.5-3B-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-3B-bnb-4bit\n",
    "# unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-3B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-3B-Instruct\n",
    "# unsloth/Qwen2.5-3B\n",
    "# unsloth/Qwen2.5-32B-bnb-4bit\n",
    "# unsloth/Qwen2.5-32B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-32B-Instruct\n",
    "# unsloth/Qwen2.5-32B\n",
    "# unsloth/Qwen2.5-14B-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-Instruct-1M-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-Instruct-1M-bnb-4bit\n",
    "# unsloth/Qwen2.5-14B-Instruct-1M\n",
    "# unsloth/Qwen2.5-14B-Instruct\n",
    "# unsloth/Qwen2.5-14B\n",
    "# unsloth/Qwen2.5-1.5B-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-1.5B-bnb-4bit\n",
    "# unsloth/Qwen2.5-1.5B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-1.5B-Instruct\n",
    "# unsloth/Qwen2.5-1.5B\n",
    "# unsloth/Qwen2.5-0.5B-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-0.5B-bnb-4bit\n",
    "# unsloth/Qwen2.5-0.5B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2.5-0.5B-Instruct\n",
    "# unsloth/Qwen2.5-0.5B\n",
    "# unsloth/Qwen2.5-0.5-bnb-4bit\n",
    "# unsloth/Qwen2-VL-7B-bnb-4bit\n",
    "# unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-VL-7B-Instruct\n",
    "# unsloth/Qwen2-VL-7B\n",
    "# unsloth/Qwen2-VL-72B-bnb-4bit\n",
    "# unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-VL-72B-Instruct\n",
    "# unsloth/Qwen2-VL-72B\n",
    "# unsloth/Qwen2-VL-2B-bnb-4bit\n",
    "# unsloth/Qwen2-VL-2B-Instruct-unsloth-bnb-4bit\n",
    "# unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-VL-2B-Instruct\n",
    "# unsloth/Qwen2-VL-2B\n",
    "# unsloth/Qwen2-Math-7B-bnb-4bit\n",
    "# unsloth/Qwen2-Math-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-Math-7B-Instruct\n",
    "# unsloth/Qwen2-Math-7B\n",
    "# unsloth/Qwen2-Math-1.5B-bnb-4bit\n",
    "# unsloth/Qwen2-Math-1.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-Math-1.5B-Instruct\n",
    "# unsloth/Qwen2-Math-1.5B\n",
    "# unsloth/Qwen2-7B-bnb-4bit\n",
    "# unsloth/Qwen2-7B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-7B-Instruct\n",
    "# unsloth/Qwen2-7B\n",
    "# unsloth/Qwen2-72B-bnb-4bit\n",
    "# unsloth/Qwen2-72B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-1.5B-bnb-4bit\n",
    "# unsloth/Qwen2-1.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-1.5B-Instruct\n",
    "# unsloth/Qwen2-1.5B\n",
    "# unsloth/Qwen2-0.5B-bnb-4bit\n",
    "# unsloth/Qwen2-0.5B-Instruct-bnb-4bit\n",
    "# unsloth/Qwen2-0.5B-Instruct\n",
    "# unsloth/Qwen2-0.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7d4c54-ded7-4f8b-bafc-62684e197bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c6409-fab2-4867-b7d7-40bba12b6b18",
   "metadata": {},
   "source": [
    "## load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41e55b6-ad9f-4f00-a3d8-fe00dab4ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.634 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "max_seq_length = 64\n",
    "dtype = None \n",
    "\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = 'unsloth/' + model_name,\n",
    "    load_in_4bit = True,\n",
    "    # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c416e01c-bc17-4afa-bc2f-e2ed09991e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(os.path.join(model_dir, 'ds_epitope'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78323b50-06e5-4ce2-acc4-b0b44870589a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3023723\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 159144\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d37469bd-c9f3-40ae-9a46-eb0d368da950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "catogories = ['No', 'Yes']\n",
    "rank_to_label = {i:n for i,n in enumerate(catogories)}\n",
    "label_to_rank = {n:i for i,n in enumerate(catogories)}\n",
    "\n",
    "# def format_data(example, tokenizer, rank_to_label):\n",
    "#     word_label = rank_to_label[example['label']]\n",
    "#     example['token'] = tokenizer.encode(word_label, add_special_tokens=False)[0]\n",
    "#     return example\n",
    "# ds = ds.map(format_data, fn_kwargs={'tokenizer': tokenizer, 'rank_to_label': rank_to_label})\n",
    "\n",
    "prompt = \"\"\"\n",
    "Here is an amino acid sequence:\n",
    "{}\n",
    "\n",
    "Is this sequence identified as Yes or No ? Answer with \"Yes\" or \"No\".\n",
    "\n",
    "SOLUTION\n",
    "The correct answer is:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6c1c4-6934-43ea-8d62-875683b87f71",
   "metadata": {},
   "source": [
    "## prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaf15d7a-f011-4cf3-b4db-c2a472ebc3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere is an amino acid sequence:\\nASVIALELL\\n\\nIs this sequence identified as Yes or No ? Answer with \"Yes\" or \"No\".\\n\\nSOLUTION\\nThe correct answer is:\\nNo\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positivelabel = \"Yes\"\n",
    "negativelabel = \"No\"\n",
    "\n",
    "yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "\n",
    "def formatting_prompts_func(dataset_):\n",
    "    # this is to fix an issue with the transformers library where the first time this function is called, it is called with a string for some reason\n",
    "    if isinstance(dataset_['text'], str):\n",
    "        return [\" \"]*100\n",
    "        \n",
    "    texts = []\n",
    "    for i in range(len(dataset_['text'])):\n",
    "        t = dataset_['text'][i]\n",
    "        label = positivelabel if dataset_['label'][i] == 1 else negativelabel\n",
    "        text = prompt.format(t, label)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "# test one\n",
    "formatting_prompts_func(ds['test'][:10])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bc14cb-1e31-4c47-b439-77d5dfa00aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training lm_head in mixed precision to save VRAM\n",
      "trainable parameters: 251838464\n"
     ]
    }
   ],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"lm_head\", # can easily be trained because it has only 2 tokens\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = 'loftq',\n",
    "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
    ")\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2046f8a5-6c8a-4994-86cf-9d899a6c3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# this custom collator is needed to change the sequence labels from yes_token_id and no_token_id to 1 and 0. It also trains only on the last token of the sequence.\n",
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # Find the last non-padding token\n",
    "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            # Set all labels to ignore_index except for the last token\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            # The old labels for the Yes and No tokens need to be mapped to 1 and 0\n",
    "            batch[\"labels\"][i, last_token_idx] = 1 if batch[\"labels\"][i, last_token_idx] == yes_token_id else 0\n",
    "\n",
    "\n",
    "        return batch\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "478b3316-b2b7-462b-adfd-a03018c300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "# 300\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    warmup_ratio = .2,\n",
    "    # num_train_epochs = 1,\n",
    "    max_steps=400,\n",
    "    learning_rate = 5e-7,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = 3407,\n",
    "    output_dir = os.path.join(model_dir, model_name, \"outputs\"),\n",
    "    # report_to = \"wandb\",\n",
    "    report_to = \"none\",\n",
    "    group_by_length = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fd5e82-b92f-4ef3-8835-e7c3b0328608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f361db17094098a85c80e7c57f03c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/3023723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_num_proc\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# not needed because group_by_length is True\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m trainer_stats = trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/trainer.py:203\u001b[39m, in \u001b[36m_backwards_compatible_trainer.<locals>.new_init\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m] = config\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bio/ml_epitope/src_llm/unsloth_compiled_cache/UnslothSFTTrainer.py:1010\u001b[39m, in \u001b[36mUnslothSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m fix_untrained_tokens(model, tokenizer, train_dataset, IGNORED_TOKENIZER_NAMES, eps = \u001b[32m1e-16\u001b[39m)\n\u001b[32m   1008\u001b[39m fix_zero_training_loss(model, tokenizer, train_dataset)\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneftune_hook_handle\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1025\u001b[39m     \u001b[38;5;28mself\u001b[39m.neftune_hook_handle.remove()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bio/ml_epitope/src_llm/unsloth_compiled_cache/UnslothSFTTrainer.py:498\u001b[39m, in \u001b[36m_UnslothSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m optimizer_cls_and_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    493\u001b[39m         warnings.warn(\n\u001b[32m    494\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe `optimizer_cls_and_kwargs` argument is only available for `transformers>=4.47.0`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe default optimizer will be used. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mRemove the `optimizer_cls_and_kwargs` or upgrade to `transformers>=4.47.0`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    497\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msuper_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33madd_model_tags\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:560\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# At this stage the model is already loaded\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_peft_model(model) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_model_quantized_and_qat_trainable:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    561\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    563\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for more details\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    564\u001b[39m     )\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_quantized_and_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _quantization_method_supports_training:\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    567\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model you are trying to fine-tune is quantized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.hf_quantizer.quantization_config.quant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m to request the support for training support for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.hf_quantizer.quantization_config.quant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds['train'],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # not needed because group_by_length is True\n",
    "    args = args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6191993-064f-4ff2-b517-a59d74ce6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_plot import TrainerPlot\n",
    "\n",
    "TrainerPlot.loss(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940928a8-9a51-43bd-b0d7-b3db8463bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saved_name = os.path.join(model_dir, model_name, 'model')\n",
    "print(saved_name)\n",
    "model.save_pretrained(saved_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996ef21-eaba-4ac5-a8d0-d1575afd924f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb55420-df43-4236-be81-168d455bbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model using \n",
    "# import pandas as pd\n",
    "# from fine_tune import FineTune\n",
    "\n",
    "# stat = ft.trainer_predict(trainer, ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d12b63-dacf-4480-ad73-ed60f4c92c51",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c83725-cdfd-4921-a4d1-44b9eb264d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903dda8-f247-474e-8f0d-47f250518adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# # max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "# max_seq_length = 50\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True\n",
    "\n",
    "# model0, tokenizer0 = FastLanguageModel.from_pretrained(\n",
    "#     '/home/yuan/bio/ml_epitope/models/llama-3-8b-bnb-4bit_epitope/llama-3-8b-bnb-4bit_epitope',\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf273bc-f0d8-4f29-86bc-a62341d4f3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70324e-decb-4524-a1f7-2e42896e5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model using \n",
    "import pandas as pd\n",
    "from fine_tune import FineTune\n",
    "\n",
    "stat = pd.DataFrame()\n",
    "ft = FineTune(model, tokenizer)\n",
    "chunk_size = 1000\n",
    "for i in range(0, 10000, chunk_size):\n",
    "    sub = ds['test'][i:i+chunk_size]\n",
    "    sub=[{'text':k, 'label':v} for k,v in zip(sub['text'], sub['label'])]\n",
    "    print(i, end=',')\n",
    "    sub_stat = ft.predict(sub, prompt, label_to_rank)\n",
    "    stat = pd.concat([stat, sub_stat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438ab51-f2ba-484a-b10d-da134b42701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(stat['predicts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dc448-86e0-44e6-965a-0b7e86fb3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_plot import TrainerPlot\n",
    "TrainerPlot.scores(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f5cc9-48c0-4fab-b1cd-3e506fb52667",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainerPlot.cm(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81ae77-6790-4be7-a0cf-1ecd448a31ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
