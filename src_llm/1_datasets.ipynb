{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f03702-c41c-498a-bb41-5905579120ef",
   "metadata": {},
   "source": [
    "## for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d340ea8b-1014-4b68-8013-c4363bd883c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/home/yuan/results/epitope'\n",
    "project_name = 'bert'\n",
    "repo_dir = os.path.dirname(os.getcwd())\n",
    "results_dir = os.path.join(repo_dir, 'models', project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6309cef6-2ac1-48e9-8bd3-2af963a97792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length 512\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained model and its tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "# binary classification: 1, 0\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "print('max_length', tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f2c0125-8ad2-4278-b5bf-494dc3980cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load data\n",
    "data_files = {\n",
    "    'train': [\n",
    "        \"/home/yuan/results/epitope/epitope.txt\",\n",
    "        \"/home/yuan/results/epitope/random.txt\",\n",
    "        \"/home/yuan/results/epitope/other.txt\",\n",
    "    ],\n",
    "}\n",
    "ds = load_dataset('csv', delimiter='\\t', column_names=['text', 'label'], data_files=data_files, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b9ff6f-e3dd-4974-be97-3c48c9e9db91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 4980441\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6345a01c-897a-4366-bdc8-3e3770416603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def update_label(example):\n",
    "    example['label'] = 1 if example['label'] == 'epitope' else 0\n",
    "    return example\n",
    "ds = ds.map(update_label)\n",
    "\n",
    "# add labels: one-hot table\n",
    "labels = np.array(pd.get_dummies(ds['label'], dtype='int')).tolist()\n",
    "labels\n",
    "ds = ds.add_column(name='labels', column=labels)\n",
    "print(ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a843a7-3e72-46c3-a5a0-6d6610c7f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RPIAEYLNTQKDM', 'label': 0, 'labels': [1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2523074-a9d6-4df3-81b4-362469ae072a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f68e5e2910b40eaaa586676ad0c857a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3598367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0471303c904abfaa5a9ccc079b8f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/635007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9462b9d738c41788bc8e4aa9bfcca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/747067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# shuffle\n",
    "ds = ds.shuffle(seed=42)\n",
    "\n",
    "# split data\n",
    "train_test = ds.train_test_split(test_size=.15)\n",
    "train_valid = train_test['train'].train_test_split(test_size=.15)\n",
    "ds = DatasetDict({\n",
    "    'train': train_valid['train'],\n",
    "    'valid': train_valid['test'],\n",
    "    'test': train_test['test'],\n",
    "})\n",
    "\n",
    "# tokenize\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "ds = ds.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f46f029-95b1-40c1-aa45-8027cc5e9422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3598367\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 635007\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 747067\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67e0908b-087f-4093-b48b-e1b97abd75a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9091c4a5-2b45-42b8-821c-95b87e16387c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5e6645ede943c8b15bbbc3ca1413e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/23 shards):   0%|          | 0/3598367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d03de15cfdd48c98f44501febe659c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/635007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976bbb1e5aae4de9a096416b8b5673db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/747067 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.save_to_disk('/home/yuan/results/epitope/epitope_bert.ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d6164-fc91-4fd1-aeda-43ddbc714fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
