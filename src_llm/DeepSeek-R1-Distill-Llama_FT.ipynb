{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4a9493-e05a-431d-8ac9-bfad6f9a42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsloth/DeepSeek-V3-bf16\n",
    "# unsloth/DeepSeek-V3-GGUF\n",
    "# unsloth/DeepSeek-V3\n",
    "# unsloth/DeepSeek-R1-Zero-GGUF\n",
    "# unsloth/DeepSeek-R1-Zero-BF16\n",
    "# unsloth/DeepSeek-R1-Zero\n",
    "# unsloth/DeepSeek-R1-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-7B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-7B\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-32B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-32B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-32B\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-14B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-14B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-14B\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Qwen-1.5B\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-8B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-8B\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-70B-unsloth-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF\n",
    "# unsloth/DeepSeek-R1-Distill-Llama-70B\n",
    "# unsloth/DeepSeek-R1-BF16\n",
    "# unsloth/DeepSeek-R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874d7de-dd10-4a0f-827a-13d76fac902b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a8c6409-fab2-4867-b7d7-40bba12b6b18",
   "metadata": {},
   "source": [
    "## load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41e55b6-ad9f-4f00-a3d8-fe00dab4ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.634 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "max_seq_length = 64\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True\n",
    "\n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c416e01c-bc17-4afa-bc2f-e2ed09991e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /home/yuan/bio/ml_epitope/models/DeepSeek-R1-Distill-Llama_epitope/ds not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m project_name = \u001b[33m\"\u001b[39m\u001b[33mDeepSeek-R1-Distill-Llama_epitope\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m results_dir = os.path.join(os.path.dirname(os.getcwd()), \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m, project_name)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m ds = \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/datasets/load.py:2140\u001b[39m, in \u001b[36mload_from_disk\u001b[39m\u001b[34m(dataset_path, keep_in_memory, storage_options)\u001b[39m\n\u001b[32m   2138\u001b[39m fs, *_ = url_to_fs(dataset_path, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m   2139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs.exists(dataset_path):\n\u001b[32m-> \u001b[39m\u001b[32m2140\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2141\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs.isfile(\n\u001b[32m   2142\u001b[39m     posixpath.join(dataset_path, config.DATASET_STATE_JSON_FILENAME)\n\u001b[32m   2143\u001b[39m ):\n\u001b[32m   2144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory /home/yuan/bio/ml_epitope/models/DeepSeek-R1-Distill-Llama_epitope/ds not found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "\n",
    "project_name = \"llm_epitope\"\n",
    "results_dir = os.path.join(os.path.dirname(os.getcwd()), 'models', project_name)\n",
    "\n",
    "ds = load_from_disk(os.path.join(results_dir, 'ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37469bd-c9f3-40ae-9a46-eb0d368da950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data\n",
    "catogories = ['No', 'Yes']\n",
    "rank_to_label = {i:n for i,n in enumerate(catogories)}\n",
    "label_to_rank = {n:i for i,n in enumerate(catogories)}\n",
    "\n",
    "# def format_data(example, tokenizer, rank_to_label):\n",
    "#     word_label = rank_to_label[example['label']]\n",
    "#     example['token'] = tokenizer.encode(word_label, add_special_tokens=False)[0]\n",
    "#     return example\n",
    "# ds = ds.map(format_data, fn_kwargs={'tokenizer': tokenizer, 'rank_to_label': rank_to_label})\n",
    "\n",
    "prompt = \"\"\"\n",
    "Here is an amino acid sequence:\n",
    "{}\n",
    "\n",
    "Is this sequence identified as Yes or No ? Answer with \"Yes\" or \"No\".\n",
    "\n",
    "SOLUTION\n",
    "The correct answer is:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78323b50-06e5-4ce2-acc4-b0b44870589a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a6c1c4-6934-43ea-8d62-875683b87f71",
   "metadata": {},
   "source": [
    "## prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf15d7a-f011-4cf3-b4db-c2a472ebc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivelabel = \"Yes\"\n",
    "negativelabel = \"No\"\n",
    "\n",
    "yes_token_id = tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n",
    "no_token_id = tokenizer.encode(\"No\", add_special_tokens=False)[0]\n",
    "\n",
    "def formatting_prompts_func(dataset_):\n",
    "    # this is to fix an issue with the transformers library where the first time this function is called, it is called with a string for some reason\n",
    "    if isinstance(dataset_['text'], str):\n",
    "        return [\" \"]*100\n",
    "        \n",
    "    texts = []\n",
    "    for i in range(len(dataset_['text'])):\n",
    "        t = dataset_['text'][i]\n",
    "        label = positivelabel if dataset_['label'][i] == 1 else negativelabel\n",
    "        text = prompt.format(t, label)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "# test one\n",
    "formatting_prompts_func(ds['test'][:10])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc14cb-1e31-4c47-b439-77d5dfa00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\n",
    "        \"lm_head\", # can easily be trained because it has only 2 tokens\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = 'loftq',\n",
    "    # loftq_config = LoftQConfig(loftq_bits = 4, loftq_iter = 1), # And LoftQ\n",
    ")\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046f8a5-6c8a-4994-86cf-9d899a6c3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# this custom collator is needed to change the sequence labels from yes_token_id and no_token_id to 1 and 0. It also trains only on the last token of the sequence.\n",
    "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        mlm: bool = False,\n",
    "        ignore_index: int = -100,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, mlm=mlm, **kwargs)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            # Find the last non-padding token\n",
    "            last_token_idx = (batch[\"labels\"][i] != self.ignore_index).nonzero()[-1].item()\n",
    "            # Set all labels to ignore_index except for the last token\n",
    "            batch[\"labels\"][i, :last_token_idx] = self.ignore_index\n",
    "            # The old labels for the Yes and No tokens need to be mapped to 1 and 0\n",
    "            batch[\"labels\"][i, last_token_idx] = 1 if batch[\"labels\"][i, last_token_idx] == yes_token_id else 0\n",
    "\n",
    "\n",
    "        return batch\n",
    "collator = DataCollatorForLastTokenLM(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b3316-b2b7-462b-adfd-a03018c300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    warmup_ratio = .2,\n",
    "    # num_train_epochs = 1,\n",
    "    max_steps=600,\n",
    "    learning_rate = 5e-7,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 10,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    seed = 3407,\n",
    "    output_dir = os.path.join(results_dir, \"outputs\"),\n",
    "    # report_to = \"wandb\",\n",
    "    report_to = \"none\",\n",
    "    group_by_length = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd5e82-b92f-4ef3-8835-e7c3b0328608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ds['train'],\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # not needed because group_by_length is True\n",
    "    args = args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6191993-064f-4ff2-b517-a59d74ce6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_plot import TrainerPlot\n",
    "\n",
    "TrainerPlot.loss(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940928a8-9a51-43bd-b0d7-b3db8463bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saved_name = os.path.join(results_dir, model_name)\n",
    "print(saved_name)\n",
    "model.save_pretrained(saved_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996ef21-eaba-4ac5-a8d0-d1575afd924f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb55420-df43-4236-be81-168d455bbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model using \n",
    "# import pandas as pd\n",
    "# from fine_tune import FineTune\n",
    "\n",
    "# stat = ft.trainer_predict(trainer, ds['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d12b63-dacf-4480-ad73-ed60f4c92c51",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c83725-cdfd-4921-a4d1-44b9eb264d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903dda8-f247-474e-8f0d-47f250518adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# # max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "# max_seq_length = 50\n",
    "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "# load_in_4bit = True\n",
    "\n",
    "# model0, tokenizer0 = FastLanguageModel.from_pretrained(\n",
    "#     '/home/yuan/bio/ml_epitope/models/llama-3-8b-bnb-4bit_epitope/llama-3-8b-bnb-4bit_epitope',\n",
    "#     load_in_4bit = load_in_4bit,\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dtype = dtype,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf273bc-f0d8-4f29-86bc-a62341d4f3cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f70324e-decb-4524-a1f7-2e42896e5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model using \n",
    "import pandas as pd\n",
    "from fine_tune import FineTune\n",
    "\n",
    "stat = pd.DataFrame()\n",
    "ft = FineTune(model, tokenizer)\n",
    "chunk_size = 1000\n",
    "for i in range(0, 10000, chunk_size):\n",
    "    sub = ds['test'][i:i+chunk_size]\n",
    "    sub=[{'text':k, 'label':v} for k,v in zip(sub['text'], sub['label'])]\n",
    "    print(i, end=',')\n",
    "    sub_stat = ft.predict(sub, prompt, label_to_rank)\n",
    "    stat = pd.concat([stat, sub_stat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438ab51-f2ba-484a-b10d-da134b42701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(stat['predicts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dc448-86e0-44e6-965a-0b7e86fb3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer_plot import TrainerPlot\n",
    "TrainerPlot.scores(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f5cc9-48c0-4fab-b1cd-3e506fb52667",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainerPlot.cm(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81ae77-6790-4be7-a0cf-1ecd448a31ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
